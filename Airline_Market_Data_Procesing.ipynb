{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWJEVLVsI/utLMFxjSnn9P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "ou2S5zlbOOnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "3-qlFXV8OY1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUqm4DcFLdSh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "path= '/content/drive/MyDrive/Tesis/' #! Path definition \n",
        "file_aero = 'Aerocivil.txt' #! File definition \n",
        "file_price = 'Precios.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantities"
      ],
      "metadata": {
        "id": "UuZjKzAiOT82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "b8O5WEivOb53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing of quantities table\n",
        "  p_f =  path + file_aero\n",
        "  data = pd.read_csv(p_f, sep=\"\\t\")\n",
        "  df =data\n",
        "  df = df.replace('(en blanco)', '0') \n",
        "\n",
        "  df =df[['Año','Mes', 'CiudadDestino','Destino','Destino_MC',\n",
        "          'NombreAeropuertoDestino','CiudadOrigen','Origen','Origen_MC', \n",
        "          'NombreAeropuertoOrigen','Leg', 'MarketLeg','Airline_Cod',\n",
        "          'NombredeEmpresa','SiglaEmpresa','Trafico','TipoVuelo','Ruta_Viva',\n",
        "          'NumerodeVuelos','PasajerosABordo','PasajerosTransito',\n",
        "          'SillasOfrecidas','KM','LF','PDEWS','Weekly_Frequencies']]\n",
        "\n",
        "  df =df.sort_values(['Año','Mes','MarketLeg']).reset_index()\n",
        "\n",
        "  df[['KM',\t'LF',\t'PDEWS','Weekly_Frequencies']]=df[['KM',\t'LF',\t'PDEWS',\n",
        "                                          'Weekly_Frequencies']].astype('string')\n",
        "\n",
        "\n",
        "  for x in ['KM',\t'LF',\t'PDEWS','Weekly_Frequencies']: \n",
        "    df[x]=df[x].apply(lambda x: x.replace(',', '.'))\n",
        "\n",
        "    \n",
        "  df[['NumerodeVuelos',\t'PasajerosABordo',\t'PasajerosTransito',\t\n",
        "      'SillasOfrecidas','KM',\t'LF',\n",
        "      'PDEWS','Weekly_Frequencies']]=df[['NumerodeVuelos',\t'PasajerosABordo',\t\n",
        "                                        'PasajerosTransito',\t'SillasOfrecidas',\n",
        "                                        'KM',\t'LF','PDEWS',\n",
        "                                        'Weekly_Frequencies']].astype('float')\n",
        "\n",
        "  df['Date'] = df['Mes'].astype('string')  + df['Año'].astype('string')\n",
        "  df['Date'] = pd.to_datetime(df['Date'], format='%m%Y')\n",
        "\n",
        "# Creating data groups\n",
        "\n",
        "  df['Company'] = df['Airline_Cod'].apply(lambda x: 'Other' if x != 'AV'and\n",
        "                                          x != 'LA' and\n",
        "                                          x != 'VH' and\n",
        "                                          x != 'VE'\n",
        "                                        else ('Avianca' if x == 'AV' \n",
        "                                            else ('Latam' if x == 'LA' \n",
        "                                              else ('Viva' if x == 'VH' \n",
        "                                                else('Easy Fly' if x=='VE' \n",
        "                                                  else 'no match')))))\n",
        "\n",
        "\n",
        "  df['Leg'] = df['Leg'].str.replace('EOH', 'MDE')\n",
        "  df['MarketLeg'] = df['MarketLeg'].str.replace('EOH', 'MDE')\n",
        "  df['Origen'] = df['Origen'].str.replace('EOH', 'MDE')\n",
        "  df['Destino'] = df['Destino'].str.replace('EOH', 'MDE')\n",
        "  df['CiudadDestino'] =df['CiudadDestino'].str.replace('RIONEGRO - ANTIOQUIA',\n",
        "                                                      'MEDELLIN')\n",
        "  df['CiudadOrigen'] =df['CiudadOrigen'].str.replace('MEDELLIN', \n",
        "                                                    'RIONEGRO - ANTIOQUIA')\n",
        "  df['Ruta_Viva'] = (df['Ruta_Viva'].str.replace('Si', '1')).astype('float')\n"
      ],
      "metadata": {
        "id": "ZN57JgW8LlXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Sampling"
      ],
      "metadata": {
        "id": "681xkF3aOfmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating df and df_full\n",
        "  # df is defined as a regular and national flight after 2021\n",
        "    df= df[(df['TipoVuelo'] == \"R\") & (df['Trafico'] == \"N\")  & (df['Año']>= 2021)].reset_index()\n",
        "  # df_full is defined as a regular and national flight at any time in the observation data (from 2011-2022)\n",
        "    df_full = df[(df['TipoVuelo'] == \"R\") & (df['Trafico'] == \"N\")].reset_index()\n",
        "\n",
        "# Changing the airline code of Ultra Air\n",
        "  df['Airline_Cod']=df['Airline_Cod'].replace('ULS', 'OL')\n",
        "\n",
        "# Deleting airlines that are not listed in the pricing data\n",
        "  df = df[(df['Airline_Cod']!= 'CM') & (df['Airline_Cod']!= 'OTROS') ]\n",
        "\n",
        "# We grouped the data to delete duplicates\n",
        "\n",
        "  df=df.groupby(['Año','Mes','Date', 'CiudadDestino','Destino','Destino_MC',\n",
        "                'CiudadOrigen','Origen','Origen_MC','Leg' , 'MarketLeg',\n",
        "                'Airline_Cod','Company','Trafico','TipoVuelo'] ,\n",
        "                as_index=False).agg({'Ruta_Viva':'sum','NumerodeVuelos':'sum', \n",
        "                                    'NumerodeVuelos':'sum',\n",
        "                                    'PasajerosABordo':'sum',\n",
        "                                    'PasajerosTransito':'sum',\n",
        "                                    'SillasOfrecidas':'sum',\n",
        "                                    'KM':'mean','LF':'mean',\n",
        "                                    'PDEWS':'mean','Weekly_Frequencies':'mean' })\n",
        "      \n",
        "\n",
        "  df_full=df_full.groupby(['Año','Mes','Date', 'CiudadDestino','Destino',\n",
        "                          'Destino_MC','CiudadOrigen','Origen','Origen_MC',\n",
        "                          'Leg', 'MarketLeg','Airline_Cod','Company' ,\n",
        "                          'Trafico','TipoVuelo'] ,\n",
        "                          as_index=False).agg({'Ruta_Viva':'sum',\n",
        "                                              'NumerodeVuelos':'sum', \n",
        "                                              'NumerodeVuelos':'sum',\n",
        "                                              'PasajerosABordo':'sum',\n",
        "                                              'PasajerosTransito':'sum',\n",
        "                                              'SillasOfrecidas':'sum',\n",
        "                                              'KM':'mean','LF':'mean',\n",
        "                                              'PDEWS':'mean',\n",
        "                                              'Weekly_Frequencies':'mean' })\n",
        "# we reduce the sample to one-year observation level\n",
        "  df['Date'] = pd.to_datetime(df['Date']).dt.to_period('M')\n",
        "  df = df[df['Date']>= '2021-02']\n",
        "\n"
      ],
      "metadata": {
        "id": "zW9pu7_xMDvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Variables and measures"
      ],
      "metadata": {
        "id": "fYa8B7LSPZfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We create the HH index for passengers on board by route and period\n",
        "  mk_share_for_hhi = df.groupby(['Date','Leg','Airline_Cod'], \n",
        "                                as_index=False)['PasajerosABordo'].agg('sum')\n",
        "                                \n",
        "  mk_share_for_hhi['id_HHI_treatment'] = mk_share_for_hhi['Date'].astype(str) + mk_share_for_hhi['Leg']\n",
        "  mk_share_for_hhi['id_m_share'] = mk_share_for_hhi['Date'].astype(str) + mk_share_for_hhi['Leg'] + mk_share_for_hhi['Airline_Cod']\n",
        "\n",
        "  consol_hhi =df.groupby(['Date', 'Leg'],\n",
        "                        as_index=False)['PasajerosABordo'].agg('sum')\n",
        "\n",
        "  consol_hhi.rename(columns = {'PasajerosABordo':'PasajerosABordo_sum'}, \n",
        "                    inplace = True)\n",
        "\n",
        "  consol_hhi['id_HHI_treatment'] = consol_hhi['Date'].astype(str) + consol_hhi['Leg']\n",
        "  mk_share_for_hhi = mk_share_for_hhi.merge(consol_hhi, on='id_HHI_treatment', \n",
        "                                            how='left')\n",
        "\n",
        "# We create the Market Share for passengers on board by airline, route and period\n",
        "  mk_share_for_hhi['m_share'] = (mk_share_for_hhi['PasajerosABordo']/ mk_share_for_hhi['PasajerosABordo_sum'])\n",
        "  df['id_m_share'] = df['Date'].astype(str) + df['Leg'] +df['Airline_Cod']\n",
        "  df_m_share = mk_share_for_hhi[['id_m_share', 'm_share']]\n",
        "  df = df.merge(df_m_share, on='id_m_share', how='left')\n",
        "\n",
        "\n",
        "\n",
        "  mk_share_for_hhi['HHI'] = (mk_share_for_hhi['m_share'].pow(2,axis=0))\n",
        "\n",
        "  mk_share_for_hhi = mk_share_for_hhi.groupby(['id_HHI_treatment','Date_x', 'Leg_x'], as_index=False)['HHI'].agg('sum')\n",
        "\n",
        "  df['id_HHI_treatment'] = df['Date'].astype(str) + df['Leg']\n",
        "  mk_share_for_hhi = mk_share_for_hhi[['id_HHI_treatment', 'HHI']]\n",
        "  df = df.merge(mk_share_for_hhi, on='id_HHI_treatment', how='left')\n",
        "\n",
        " \n",
        "# We create the future treatment and pre-post variables by  route and period\n",
        "  df['OL_Presence'] =np.where(df['Airline_Cod'] == 'OL', 1, 0)\n",
        "  df['OL_entrance'] =np.where(df['Date'] > '2022-03', 1, 0)\n",
        "\n",
        "\n",
        "  treatment = df.groupby(['Date','Leg'], \n",
        "                        as_index=False)['OL_Presence','OL_entrance'].agg('sum')\n",
        "  TREATMENT = ['OL_Presence','OL_entrance']\n",
        "\n",
        "  for i in TREATMENT:\n",
        "    treatment[i] = np.where(treatment[i]>=1,1,0)\n",
        "\n",
        "  df = df.drop(columns=['OL_Presence','OL_entrance'])\n",
        "\n",
        "\n",
        "  treatment_2 = treatment.groupby('Leg', as_index = False)['OL_Presence'].agg(sum)\n",
        "  TREATMENT_2 = ['OL_Presence']\n",
        "\n",
        "\n",
        "  for i in TREATMENT_2:\n",
        "    treatment_2[i] = np.where(treatment_2[i]>=1,1,0)\n",
        "    \n",
        "\n",
        "  treatment = treatment[[ 'OL_entrance', 'Leg', 'Date']]\n",
        "  treatment = treatment.merge(treatment_2, on='Leg', how='left')\n",
        "\n",
        "  treatment['id_HHI_treatment'] = treatment['Date'].astype(str) + treatment['Leg']\n",
        "  treatment = treatment[['id_HHI_treatment', 'OL_Presence','OL_entrance']]\n",
        "\n",
        "  df = df.merge(treatment, on='id_HHI_treatment', how='left')\n"
      ],
      "metadata": {
        "id": "fIY9SmEoPQO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prices"
      ],
      "metadata": {
        "id": "W02PW4oXTH0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning and sampling"
      ],
      "metadata": {
        "id": "otLxUGtqVB7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Processing of prices table\n",
        "  p_f_p =  path + file_price\n",
        "  df_p = pd.read_csv(p_f_p, sep=\"\\t\")\n",
        "\n",
        "  df_p[['fullprice',\t'Netprice', 'tax']]=df_p[['fullprice',\t'Netprice', 'tax']].astype('string')\n",
        "\n",
        "  for x in ['fullprice',\t'Netprice', 'tax']: \n",
        "    df_p[x]=df_p[x].apply(lambda x: x.replace(',', '.'))\n",
        "\n",
        "  df_p[['fullprice',\t'Netprice', 'tax']]=round(df_p[['fullprice',\t'Netprice', 'tax']].astype('float'),2)\n",
        "\n",
        "  df_p['ObservationDate'] = pd.to_datetime(df_p['ObservationDate'], format='%Y-%m-%d')\n",
        "  df_p['Depdate'] = pd.to_datetime(df_p['Depdate'], format='%Y-%m-%d')\n",
        "\n",
        "  df_p['Año'], df_p['Mes'], df_p['Dia'] = pd.to_datetime(df_p['Depdate']).dt.year, pd.to_datetime(df_p['Depdate']).dt.month, pd.to_datetime(df_p['Depdate']).dt.day\n",
        "  df_p['Month-Year'] =  pd.to_datetime(df_p['Depdate']).dt.to_period('M')\n",
        "\n",
        "  df_p['Leg'] = df_p['Leg'].str.replace('EOH', 'MDE')\n",
        "  df_p =df_p.sort_values(['Año','Mes','Leg']).reset_index()\n",
        "\n",
        "# We sampling data from 2021-08 to 2022-08\n",
        "  df_p = df_p[(df_p['Month-Year']<='2022-08')]\n"
      ],
      "metadata": {
        "id": "64h_yvlLTOy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New Variables and Data grouping"
      ],
      "metadata": {
        "id": "KkwDcYk7U4B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a new variable that indicates the number of days for the flight departure. \n",
        "  df_p['Days for depature'] = ( df_p['ObservationDate']-df_p['Depdate'] )\n",
        "  df_p =df_p.sort_values(['Depdate','Days for depature'])\n",
        "  df_p['Days for depature']= df_p['Days for depature'].dt.days\n",
        "\n",
        "# We grouped by month \n",
        "  df_p = df_p.groupby(['Año','Mes','Month-Year','Carrier','Leg','Days for depature'], as_index=False)['Netprice','fullprice','tax'].agg('mean')"
      ],
      "metadata": {
        "id": "8SptpOsLU9g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining quantities and prices"
      ],
      "metadata": {
        "id": "P3bYlB_UWKj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define new data frames\n",
        "  df_APP1 = df\n",
        "  df_p_month_day_APP1 = df_p\n"
      ],
      "metadata": {
        "id": "5GrTbv9TVmKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We pivot the price data\n",
        "  df_p_month_day_pivot_APP1 = df_p_month_day_APP1.pivot( index =[ 'Año','Mes','Month-Year','Carrier',\"Leg\"], columns=['Days for depature'], values=['fullprice'])\n",
        "  df_p_month_day_pivot_APP1.columns = df_p_month_day_pivot_APP1.columns.droplevel(0) \n",
        "  df_p_month_day_pivot_APP1.columns.name = None\n",
        "  df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1.reset_index()\n",
        "\n",
        "# We create an ID code (year + month + airline + route ) \n",
        "  df_p_month_day_pivot_APP1['id'] = df_p_month_day_pivot_APP1['Año'].astype(str) + df_p_month_day_pivot_APP1['Mes'].astype(str) + df_p_month_day_pivot_APP1['Carrier'] +df_p_month_day_pivot_APP1['Leg']\n",
        "  df_APP1['id'] = df_APP1['Año'].astype(str) + df_APP1['Mes'].astype(str) + df_APP1['Airline_Cod'] +df_APP1['Leg']\n",
        "\n",
        "# We condition the presence of data in our samples in order to develop the join exercise further.\n",
        "  df_APP1 = df_APP1[df_APP1['Leg'].isin(list(df_p.groupby('Leg', as_index = False).count()['Leg']))]\n",
        "  df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1[df_p_month_day_pivot_APP1['id'].isin(list(df_APP1.groupby('id', as_index = False).count()['id']))]\n",
        "  df_APP1 = df_APP1[df_APP1['id'].isin(list(df_p_month_day_pivot_APP1.groupby('id', as_index = False).count()['id']))]\n",
        "\n",
        "# We drop some columns that we do not use\n",
        "  df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1.drop(columns=['Año','Mes','Month-Year','Carrier','Leg'])\n",
        "\n",
        "# We join the Data\n",
        "  full_data_APP1 = pd.merge(df_APP1,df_p_month_day_pivot_APP1, on=[\"id\",\"id\"])\n",
        "  full_data_APP1= full_data_APP1.sort_values(['Date','Leg'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_eEI95ABW9oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data processing verification and export \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5lgycvXlayPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  if full_data_APP1['id'].nunique()/ len(full_data_APP1)==1 :\n",
        "\n",
        "    # To excel\n",
        "    full_data_APP1.to_excel('Data_for_regresions_APP_1.xlsx')\n",
        "    # To stata\n",
        "    full_data_APP1_to_stata = full_data_APP1\n",
        "    full_data_APP1_to_stata['Date'] = full_data_APP1_to_stata['Año'].astype(str)+'-' +full_data_APP1_to_stata['Mes'].astype(str)\n",
        "    full_data_APP1_to_stata.to_stata('Data_for_regresions_APP_1.dta')\n",
        "\n",
        "    print('The join exercise was successfully developed. We do not have duplicates')\n",
        "\n",
        "  else:\n",
        "    print(\"We need to review de data processing. An error was made\")"
      ],
      "metadata": {
        "id": "4wU8NJG8auLB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
