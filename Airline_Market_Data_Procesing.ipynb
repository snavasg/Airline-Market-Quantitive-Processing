{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2S5zlbOOnX"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qlFXV8OY1w"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pUqm4DcFLdSh"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "import os\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "\n",
        "path= os.path.abspath(glob.glob(\"**\")[1]).replace(\"\\\\\", '/' ) + \"/\" #! Path definition \n",
        "file_aero = 'Aerocivil.txt' #! File definition \n",
        "file_price = 'Precios.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuZjKzAiOT82"
      },
      "source": [
        "## Quantities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8O5WEivOb53"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZN57JgW8LlXW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\navas.LAPTOP-5SHC5PM9\\AppData\\Local\\Temp\\ipykernel_23624\\4154963715.py:3: DtypeWarning: Columns (3,4,5,18,24,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(p_f, sep=\"\\t\")\n"
          ]
        }
      ],
      "source": [
        "# Processing of quantities table\n",
        "p_f =  path + file_aero\n",
        "data = pd.read_csv(p_f, sep=\"\\t\")\n",
        "df =data\n",
        "df = df.replace('(en blanco)', '0') \n",
        "\n",
        "df =df[['Año','Mes', 'CiudadDestino','Destino','Destino_MC',\n",
        "        'NombreAeropuertoDestino','CiudadOrigen','Origen','Origen_MC', \n",
        "        'NombreAeropuertoOrigen','Leg', 'MarketLeg','Airline_Cod',\n",
        "        'NombredeEmpresa','SiglaEmpresa','Trafico','TipoVuelo','Ruta_Viva',\n",
        "        'NumerodeVuelos','PasajerosABordo','PasajerosTransito',\n",
        "        'SillasOfrecidas','KM','LF','PDEWS','Weekly_Frequencies']]\n",
        "\n",
        "df =df.sort_values(['Año','Mes','MarketLeg']).reset_index()\n",
        "\n",
        "df[['KM',\t'LF',\t'PDEWS','Weekly_Frequencies']]=df[['KM',\t'LF',\t'PDEWS',\n",
        "                                        'Weekly_Frequencies']].astype('string')\n",
        "\n",
        "\n",
        "for x in ['KM',\t'LF',\t'PDEWS','Weekly_Frequencies']: \n",
        "  df[x]=df[x].apply(lambda x: x.replace(',', '.'))\n",
        "\n",
        "  \n",
        "df[['NumerodeVuelos',\t'PasajerosABordo',\t'PasajerosTransito',\t\n",
        "    'SillasOfrecidas','KM',\t'LF',\n",
        "    'PDEWS','Weekly_Frequencies']]=df[['NumerodeVuelos',\t'PasajerosABordo',\t\n",
        "                                      'PasajerosTransito',\t'SillasOfrecidas',\n",
        "                                      'KM',\t'LF','PDEWS',\n",
        "                                      'Weekly_Frequencies']].astype('float')\n",
        "\n",
        "df['Date'] = df['Mes'].astype('string')  + df['Año'].astype('string')\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%m%Y')\n",
        "\n",
        "# Creating data groups\n",
        "\n",
        "df['Company'] = df['Airline_Cod'].apply(lambda x: 'Other' if x != 'AV'and\n",
        "                                        x != 'LA' and\n",
        "                                        x != 'VH' and\n",
        "                                        x != 'VE'\n",
        "                                      else ('Avianca' if x == 'AV' \n",
        "                                          else ('Latam' if x == 'LA' \n",
        "                                            else ('Viva' if x == 'VH' \n",
        "                                              else('Easy Fly' if x=='VE' \n",
        "                                                else 'no match')))))\n",
        "\n",
        "\n",
        "df['Leg'] = df['Leg'].str.replace('EOH', 'MDE')\n",
        "df['MarketLeg'] = df['MarketLeg'].str.replace('EOH', 'MDE')\n",
        "df['Origen'] = df['Origen'].str.replace('EOH', 'MDE')\n",
        "df['Destino'] = df['Destino'].str.replace('EOH', 'MDE')\n",
        "df['CiudadDestino'] =df['CiudadDestino'].str.replace('RIONEGRO - ANTIOQUIA',\n",
        "                                                    'MEDELLIN')\n",
        "df['CiudadOrigen'] =df['CiudadOrigen'].str.replace('MEDELLIN', \n",
        "                                                  'RIONEGRO - ANTIOQUIA')\n",
        "df['Ruta_Viva'] = (df['Ruta_Viva'].str.replace('Si', '1')).astype('float')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681xkF3aOfmS"
      },
      "source": [
        "### Data Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "zW9pu7_xMDvE"
      },
      "outputs": [],
      "source": [
        "# Creating df and df_full\n",
        "# df is defined as a regular and national flight after 2021\n",
        "df= df[(df['TipoVuelo'] == \"R\") & (df['Trafico'] == \"N\")  & (df['Año']>= 2021)].reset_index()\n",
        "# Dropping Level_0\n",
        "df.drop(['level_0'], axis=1, inplace = True)\n",
        "\n",
        "# df_full is defined as a regular and national flight at any time in the observation data (from 2011-2022)\n",
        "df_full = df[(df['TipoVuelo'] == \"R\") & (df['Trafico'] == \"N\")].reset_index()\n",
        "\n",
        "# Changing the airline code of Ultra Air\n",
        "df['Airline_Cod']=df['Airline_Cod'].replace('ULS', 'OL')\n",
        "\n",
        "# Deleting airlines that are not listed in the pricing data\n",
        "df = df[(df['Airline_Cod']!= 'CM') & (df['Airline_Cod']!= 'OTROS') ]\n",
        "\n",
        "# We grouped the data to delete duplicates\n",
        "\n",
        "df=df.groupby(['Año','Mes','Date', 'CiudadDestino','Destino','Destino_MC',\n",
        "            'CiudadOrigen','Origen','Origen_MC','Leg' , 'MarketLeg',\n",
        "            'Airline_Cod','Company','Trafico','TipoVuelo'] ,\n",
        "            as_index=False).agg({'Ruta_Viva':'sum','NumerodeVuelos':'sum', \n",
        "                                'NumerodeVuelos':'sum',\n",
        "                                'PasajerosABordo':'sum',\n",
        "                                'PasajerosTransito':'sum',\n",
        "                                'SillasOfrecidas':'sum',\n",
        "                                'KM':'mean','LF':'mean',\n",
        "                                'PDEWS':'mean','Weekly_Frequencies':'mean' })\n",
        "    \n",
        "\n",
        "df_full=df_full.groupby(['Año','Mes','Date', 'CiudadDestino','Destino',\n",
        "                        'Destino_MC','CiudadOrigen','Origen','Origen_MC',\n",
        "                        'Leg', 'MarketLeg','Airline_Cod','Company' ,\n",
        "                        'Trafico','TipoVuelo'] ,\n",
        "                        as_index=False).agg({'Ruta_Viva':'sum',\n",
        "                                            'NumerodeVuelos':'sum', \n",
        "                                            'NumerodeVuelos':'sum',\n",
        "                                            'PasajerosABordo':'sum',\n",
        "                                            'PasajerosTransito':'sum',\n",
        "                                            'SillasOfrecidas':'sum',\n",
        "                                            'KM':'mean','LF':'mean',\n",
        "                                            'PDEWS':'mean',\n",
        "                                            'Weekly_Frequencies':'mean' })\n",
        "# we reduce the sample to one-year observation level\n",
        "df['Date'] = pd.to_datetime(df['Date']).dt.to_period('M')\n",
        "df = df[df['Date']>= '2021-02']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYa8B7LSPZfy"
      },
      "source": [
        "### New Variables and measures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "fIY9SmEoPQO2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\navas.LAPTOP-5SHC5PM9\\AppData\\Local\\Temp\\ipykernel_23624\\3825487084.py:40: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  treatment = df.groupby(['Date','Leg'],\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# We create the HH index for passengers on board by route and period\n",
        "mk_share_for_hhi = df.groupby(['Date','Leg','Airline_Cod'], \n",
        "                              as_index=False)['PasajerosABordo'].agg('sum')\n",
        "                              \n",
        "mk_share_for_hhi['id_HHI_treatment'] = mk_share_for_hhi['Date'].astype(str) + mk_share_for_hhi['Leg']\n",
        "mk_share_for_hhi['id_m_share'] = mk_share_for_hhi['Date'].astype(str) + mk_share_for_hhi['Leg'] + mk_share_for_hhi['Airline_Cod']\n",
        "\n",
        "consol_hhi =df.groupby(['Date', 'Leg'],\n",
        "                      as_index=False)['PasajerosABordo'].agg('sum')\n",
        "\n",
        "consol_hhi.rename(columns = {'PasajerosABordo':'PasajerosABordo_sum'}, \n",
        "                  inplace = True)\n",
        "\n",
        "consol_hhi['id_HHI_treatment'] = consol_hhi['Date'].astype(str) + consol_hhi['Leg']\n",
        "mk_share_for_hhi = mk_share_for_hhi.merge(consol_hhi, on='id_HHI_treatment', \n",
        "                                          how='left')\n",
        "\n",
        "# We create the Market Share for passengers on board by airline, route and period\n",
        "mk_share_for_hhi['m_share'] = (mk_share_for_hhi['PasajerosABordo']/ mk_share_for_hhi['PasajerosABordo_sum'])\n",
        "df['id_m_share'] = df['Date'].astype(str) + df['Leg'] +df['Airline_Cod']\n",
        "df_m_share = mk_share_for_hhi[['id_m_share', 'm_share']]\n",
        "df = df.merge(df_m_share, on='id_m_share', how='left')\n",
        "\n",
        "\n",
        "\n",
        "mk_share_for_hhi['HHI'] = (mk_share_for_hhi['m_share'].pow(2,axis=0))\n",
        "\n",
        "mk_share_for_hhi = mk_share_for_hhi.groupby(['id_HHI_treatment','Date_x', 'Leg_x'], as_index=False)['HHI'].agg('sum')\n",
        "\n",
        "df['id_HHI_treatment'] = df['Date'].astype(str) + df['Leg']\n",
        "mk_share_for_hhi = mk_share_for_hhi[['id_HHI_treatment', 'HHI']]\n",
        "df = df.merge(mk_share_for_hhi, on='id_HHI_treatment', how='left')\n",
        "\n",
        "\n",
        "# We create the future treatment and pre-post variables by  route and period\n",
        "df['OL_Presence'] =np.where(df['Airline_Cod'] == 'OL', 1, 0)\n",
        "df['OL_entrance'] =np.where(df['Date'] > '2022-03', 1, 0)\n",
        "\n",
        "\n",
        "treatment = df.groupby(['Date','Leg'], \n",
        "                      as_index=False)['OL_Presence','OL_entrance'].agg('sum')\n",
        "TREATMENT = ['OL_Presence','OL_entrance']\n",
        "\n",
        "for i in TREATMENT:\n",
        "  treatment[i] = np.where(treatment[i]>=1,1,0)\n",
        "\n",
        "df = df.drop(columns=['OL_Presence','OL_entrance'])\n",
        "\n",
        "\n",
        "treatment_2 = treatment.groupby('Leg', as_index = False)['OL_Presence'].agg(sum)\n",
        "TREATMENT_2 = ['OL_Presence']\n",
        "\n",
        "\n",
        "for i in TREATMENT_2:\n",
        "  treatment_2[i] = np.where(treatment_2[i]>=1,1,0)\n",
        "  \n",
        "\n",
        "treatment = treatment[[ 'OL_entrance', 'Leg', 'Date']]\n",
        "treatment = treatment.merge(treatment_2, on='Leg', how='left')\n",
        "\n",
        "treatment['id_HHI_treatment'] = treatment['Date'].astype(str) + treatment['Leg']\n",
        "treatment = treatment[['id_HHI_treatment', 'OL_Presence','OL_entrance']]\n",
        "\n",
        "df = df.merge(treatment, on='id_HHI_treatment', how='left')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02PW4oXTH0N"
      },
      "source": [
        "## Prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otLxUGtqVB7x"
      },
      "source": [
        "### Data Cleaning and sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "64h_yvlLTOy1"
      },
      "outputs": [],
      "source": [
        "#  Processing of prices table\n",
        "p_f_p =  path + file_price\n",
        "df_p = pd.read_csv(p_f_p, sep=\"\\t\")\n",
        "\n",
        "df_p[['fullprice',\t'Netprice', 'tax']]=df_p[['fullprice',\t'Netprice', 'tax']].astype('string')\n",
        "\n",
        "for x in ['fullprice',\t'Netprice', 'tax']: \n",
        "  df_p[x]=df_p[x].apply(lambda x: x.replace(',', '.'))\n",
        "\n",
        "df_p[['fullprice',\t'Netprice', 'tax']]=round(df_p[['fullprice',\t'Netprice', 'tax']].astype('float'),2)\n",
        "\n",
        "df_p['ObservationDate'] = pd.to_datetime(df_p['ObservationDate'], format='%Y-%m-%d')\n",
        "df_p['Depdate'] = pd.to_datetime(df_p['Depdate'], format='%Y-%m-%d')\n",
        "\n",
        "df_p['Año'], df_p['Mes'], df_p['Dia'] = pd.to_datetime(df_p['Depdate']).dt.year, pd.to_datetime(df_p['Depdate']).dt.month, pd.to_datetime(df_p['Depdate']).dt.day\n",
        "df_p['Month-Year'] =  pd.to_datetime(df_p['Depdate']).dt.to_period('M')\n",
        "\n",
        "df_p['Leg'] = df_p['Leg'].str.replace('EOH', 'MDE')\n",
        "df_p =df_p.sort_values(['Año','Mes','Leg']).reset_index()\n",
        "\n",
        "# We sampling data from 2021-08 to 2022-08\n",
        "df_p = df_p[(df_p['Month-Year']<='2022-08')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkwDcYk7U4B0"
      },
      "source": [
        "### New Variables and Data grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8SptpOsLU9g3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\navas.LAPTOP-5SHC5PM9\\AppData\\Local\\Temp\\ipykernel_23624\\849820958.py:7: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  df_p = df_p.groupby(['Año','Mes','Month-Year','Carrier','Leg','Days for depature'], as_index=False)['Netprice','fullprice','tax'].agg('mean')\n"
          ]
        }
      ],
      "source": [
        "# We create a new variable that indicates the number of days for the flight departure. \n",
        "df_p['Days for depature'] = ( df_p['ObservationDate']-df_p['Depdate'] )\n",
        "df_p =df_p.sort_values(['Depdate','Days for depature'])\n",
        "df_p['Days for depature']= df_p['Days for depature'].dt.days\n",
        "\n",
        "# We grouped by month \n",
        "df_p = df_p.groupby(['Año','Mes','Month-Year','Carrier','Leg','Days for depature'], as_index=False)['Netprice','fullprice','tax'].agg('mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3bYlB_UWKj9"
      },
      "source": [
        "## Joining quantities and prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "5GrTbv9TVmKc"
      },
      "outputs": [],
      "source": [
        "# We define new data frames\n",
        "df_APP1 = df\n",
        "df_p_month_day_APP1 = df_p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_eEI95ABW9oX"
      },
      "outputs": [],
      "source": [
        "# We pivot the price data\n",
        "df_p_month_day_pivot_APP1 = df_p_month_day_APP1.pivot( index =[ 'Año','Mes','Month-Year','Carrier',\"Leg\"], columns=['Days for depature'], values=['fullprice'])\n",
        "df_p_month_day_pivot_APP1.columns = df_p_month_day_pivot_APP1.columns.droplevel(0) \n",
        "df_p_month_day_pivot_APP1.columns.name = None\n",
        "df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1.reset_index()\n",
        "\n",
        "# We create an ID code (year + month + airline + route ) \n",
        "df_p_month_day_pivot_APP1['id'] = df_p_month_day_pivot_APP1['Año'].astype(str) + df_p_month_day_pivot_APP1['Mes'].astype(str) + df_p_month_day_pivot_APP1['Carrier'] +df_p_month_day_pivot_APP1['Leg']\n",
        "df_APP1['id'] = df_APP1['Año'].astype(str) + df_APP1['Mes'].astype(str) + df_APP1['Airline_Cod'] +df_APP1['Leg']\n",
        "\n",
        "# We condition the presence of data in our samples in order to develop the join exercise further.\n",
        "df_APP1 = df_APP1[df_APP1['Leg'].isin(list(df_p.groupby('Leg', as_index = False).count()['Leg']))]\n",
        "df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1[df_p_month_day_pivot_APP1['id'].isin(list(df_APP1.groupby('id', as_index = False).count()['id']))]\n",
        "df_APP1 = df_APP1[df_APP1['id'].isin(list(df_p_month_day_pivot_APP1.groupby('id', as_index = False).count()['id']))]\n",
        "\n",
        "# We drop some columns that we do not use\n",
        "df_p_month_day_pivot_APP1 = df_p_month_day_pivot_APP1.drop(columns=['Año','Mes','Month-Year','Carrier','Leg'])\n",
        "\n",
        "# We join the Data\n",
        "full_data_APP1 = pd.merge(df_APP1,df_p_month_day_pivot_APP1, on=[\"id\",\"id\"])\n",
        "full_data_APP1= full_data_APP1.sort_values(['Date','Leg'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lgycvXlayPq"
      },
      "source": [
        "### Data processing verification and export \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:/Users/navas.LAPTOP-5SHC5PM9/OneDrive - Universidad EAFIT/GitHub/Colombian-Air-Market-Data-Processing/Data_for_regresions_APP_1.xlsx'"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "os.path.abspath(glob.glob('*/')[0]).replace(\"\\\\\", '/' ).replace(\"/Data\", '/Data_for_regresions_APP_1.xlsx' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wU8NJG8auLB"
      },
      "outputs": [],
      "source": [
        "\n",
        "if full_data_APP1['id'].nunique()/ len(full_data_APP1)==1 :\n",
        "\n",
        "  # To excel\n",
        "  full_data_APP1.to_excel(os.path.abspath(glob.glob('*/')[0]).replace(\"\\\\\", '/' ).replace(\"/Data\", '/Data_for_regresions_APP_1.xlsx' ))\n",
        "  # To stata\n",
        "  full_data_APP1_to_stata = full_data_APP1\n",
        "  full_data_APP1_to_stata['Date'] = full_data_APP1_to_stata['Año'].astype(str)+'-' +full_data_APP1_to_stata['Mes'].astype(str)\n",
        "  full_data_APP1_to_stata.to_stata(os.path.abspath(glob.glob('*/')[0]).replace(\"\\\\\", '/' ).replace(\"/Data\", '/Data_for_regresions_APP_1.dta' ))\n",
        "\n",
        "  print('The join exercise was successfully developed. We do not have duplicates')\n",
        "\n",
        "else:\n",
        "  print(\"We need to review de data processing. An error was made\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STATA CODES AND PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  ___  ____  ____  ____  ____ ®\n",
            " /__    /   ____/   /   ____/      17.0\n",
            "___/   /   /___/   /   /___/       MP—Parallel Edition\n",
            "\n",
            " Statistics and Data Science       Copyright 1985-2021 StataCorp LLC\n",
            "                                   StataCorp\n",
            "                                   4905 Lakeway Drive\n",
            "                                   College Station, Texas 77845 USA\n",
            "                                   800-STATA-PC        https://www.stata.com\n",
            "                                   979-696-4600        stata@stata.com\n",
            "\n",
            "Stata license: Unlimited-user 2-core network, expiring  6 Oct 2023\n",
            "Serial number: 501709313513\n",
            "  Licensed to: SANTIAGO NAVAS\n",
            "               EAFIT\n",
            "\n",
            "Notes:\n",
            "      1. Unicode is supported; see help unicode_advice.\n",
            "      2. More than 2 billion observations are allowed; see help obs_advice.\n",
            "      3. Maximum number of variables is set to 5,000; see help set_maxvar.\n"
          ]
        }
      ],
      "source": [
        "import stata_setup\n",
        "#For windows \n",
        "stata_setup.config('C:/Program Files/Stata17/', 'mp')\n",
        "\n",
        "# For linux 'uncomment' the following line\n",
        "#stata_setup.config(\"/usr/local/stata17/\", \"mp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\navas.LAPTOP-5SHC5PM9\\OneDrive - Universidad EAFIT\\GitHub\\Colombian-Ai\n",
            "> r-Market-Data-Processing\n"
          ]
        }
      ],
      "source": [
        "%%stata\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMWJEVLVsI/utLMFxjSnn9P",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "14da4635d9bc1c91defc0cda200d7b8cd379b4a348c119f5267a6ff93be5824f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
